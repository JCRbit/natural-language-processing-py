{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Corpus Preprocessing"
      ],
      "metadata": {
        "id": "plVS4xGmQK4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Load data"
      ],
      "metadata": {
        "id": "FhzneexcPBny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fn-oD908HamS",
        "outputId": "fbb79874-259b-4427-893f-8be07226b092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "base_path = '/content/drive/MyDrive/nlp/amazon'\n",
        "reviews_train_df = pd.read_csv(base_path + '/data/pet_supplies_train.csv')\n",
        "reviews_test_df = pd.read_csv(base_path + '/data/pet_supplies_test.csv')"
      ],
      "metadata": {
        "id": "O3drKXQaHJgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Preprocess data"
      ],
      "metadata": {
        "id": "01CnQ3BWPJPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI8IpvlH6Lm7",
        "outputId": "2a4ee949-d2c0-41ff-da43-01bbfac5b868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=9d59934a334ea760437eb6e7ad24ea39a69fea4ae4a8f390c7bbfcdda745bffc\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCVIKnkMomE",
        "outputId": "1aba7b30-a79e-4f8f-b6f1-f04325e86c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map Treebank tags to WordNet tags\n",
        "tag_map = {'CD': wordnet.NOUN,\n",
        "           'EX': wordnet.ADV,\n",
        "           'IN': wordnet.ADV,\n",
        "           'JJ': wordnet.ADJ,\n",
        "           'NN': wordnet.NOUN,\n",
        "           'PD': wordnet.ADJ,\n",
        "           'RB': wordnet.ADV,\n",
        "           'RP': wordnet.ADJ,\n",
        "           'VB': wordnet.VERB\n",
        "          }\n",
        "\n",
        "def wordnet_pos_tag(tokens, tag_map=tag_map):\n",
        "  '''Tag the given list of tokens using WordNet tags.'''\n",
        "  return [(token, tag_map.get(treebank_tag[:2])) for token, treebank_tag in nltk.pos_tag(tokens)]"
      ],
      "metadata": {
        "id": "Y11XLp0Bwe21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(token, tag, lemmatizer):\n",
        "  '''Lemmatize token with POS tag.'''\n",
        "  return token if tag is None else lemmatizer.lemmatize(token, tag)"
      ],
      "metadata": {
        "id": "M7QuAC61wnpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_words(token):\n",
        "  return num2words(token) if token.isdigit() else token"
      ],
      "metadata": {
        "id": "XCy8xQJdVDbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline\n",
        "def clean_text(text,\n",
        "               tokenizer=nltk.word_tokenize,\n",
        "               punctuation=punctuation,\n",
        "               stopwords=stopwords.words('english'),\n",
        "               lemmatizer=WordNetLemmatizer()):\n",
        "  words = []\n",
        "  # Convert text to lowercase, tokenize and tag text\n",
        "  tokens_tags = wordnet_pos_tag(tokenizer(text.lower()))\n",
        "  for token, tag in tokens_tags:\n",
        "    # Remove punctuation marks\n",
        "    if token not in punctuation:\n",
        "      tk = token.translate(str.maketrans('', '', punctuation))\n",
        "      # Lemmatize word\n",
        "      lmtword = lemmatization(tk, tag, lemmatizer)\n",
        "      # Remove stopwords\n",
        "      if lmtword not in stopwords and lmtword != '':\n",
        "        # Convert numbers to words\n",
        "        words.append(num_to_words(lmtword))\n",
        "  return ' '.join(words)"
      ],
      "metadata": {
        "id": "Lbo3gMqBKq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_df['preprocReviewText'] = reviews_train_df['reviewText'].map(clean_text)\n",
        "reviews_train_df.drop(reviews_train_df[reviews_train_df['preprocReviewText'] == ''].index, inplace=True)\n",
        "\n",
        "reviews_test_df['preprocReviewText'] = reviews_test_df['reviewText'].map(clean_text)\n",
        "reviews_test_df.drop(reviews_test_df[reviews_test_df['preprocReviewText'] == ''].index, inplace=True)"
      ],
      "metadata": {
        "id": "547Y4xf-Ceae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_df = reviews_train_df[['sentiment', 'preprocReviewText', 'reviewText']]\n",
        "reviews_train_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFfGfqibAJ8",
        "outputId": "e1b5fe94-a16c-4433-c591-1200edb7f46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 16226 entries, 0 to 16232\n",
            "Data columns (total 3 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   sentiment          16226 non-null  int64 \n",
            " 1   preprocReviewText  16226 non-null  object\n",
            " 2   reviewText         16226 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 507.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_test_df = reviews_test_df[['sentiment', 'preprocReviewText', 'reviewText']]\n",
        "reviews_test_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLXRVJ0CyB_f",
        "outputId": "f38de95a-8482-4eae-9a36-dcfbb80c4807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4057 entries, 0 to 4058\n",
            "Data columns (total 3 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   sentiment          4057 non-null   int64 \n",
            " 1   preprocReviewText  4057 non-null   object\n",
            " 2   reviewText         4057 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 126.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Save preprocessed data"
      ],
      "metadata": {
        "id": "1wSinrbMPTJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_train_df.to_csv(base_path + '/data/preproc_pet_supplies_train.csv', index=False)\n",
        "reviews_test_df.to_csv(base_path + '/data/preproc_pet_supplies_test.csv', index=False)"
      ],
      "metadata": {
        "id": "mhKV7Z8ADf7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Comments"
      ],
      "metadata": {
        "id": "uTavgGCpPYlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen del preprocesamiento del corpus:\n",
        "1. Transformación a minúsculas.\n",
        "2. Tokenizacion con `TreebankWordTokenizer`.\n",
        "3. Etiquetado de tokens con el objetivo de hacer una mejor lematización, ha habido que transformar las etiquetas por defecto (Treebank) a las etiquetas compatibles con el lematizador (WordNet).\n",
        "4. Eliminación de signos de puntuación.\n",
        "5. Lematización con `WordNetLemmatizer`.\n",
        "6. Eliminación de stopwords.\n",
        "7. Transformación de números a palabras."
      ],
      "metadata": {
        "id": "i7aWgbEhVuaq"
      }
    }
  ]
}